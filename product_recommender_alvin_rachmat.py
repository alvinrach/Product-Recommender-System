# -*- coding: utf-8 -*-
"""Sub 2 Product Recommender - Alvin Rachmat.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zJe3JyV4z5xV1F7RwaRZudfPPyWN9jeY

# Mengimpor Data Dengan API Kaggle

Pastikan sudah melakukan *sign up* pada Kaggle dan mengetahui bagaimana cara memperoleh kredensial untuk menjalankan API Kaggle
"""

!mkdir -p ~/.kaggle
!echo '{"username":"alvinrach","key":"01d0bc7e09928966a5b93eabab748737"}' > ~/.kaggle/kaggle.json
!chmod 600 ~/.kaggle/kaggle.json
!kaggle datasets download -d ruchi798/marketing-bias-in-product-recommendations

"""Melakukan unzip pada data yang telah didownload"""

!unzip /content/marketing-bias-in-product-recommendations.zip

"""# Mengimpor Packages yang Dibutuhkan

Lakukan impor pada dependensi yang dibutuhkan
"""

import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras import layers
from tensorflow import keras
import matplotlib.pyplot as plt
import seaborn as sns

"""Berikut merupakan data yang digunakan"""

d = pd.read_csv('/content/electronics.csv')
d

"""Melakukan pengecekan data terduplikasi. Tidak terdapat data terduplikasi"""

d[d.duplicated()]

"""Melakukan pengecekan *null values* dan *datatype*"""

d.info()

"""Melakukan pengecekan fitur split"""

d.split.value_counts()

"""Melihat jumlah item pada dataset. Terdapat 9560 item"""

len(d.item_id.unique())

"""Melihat jumlah user pada dataset. Terdapat 1157633 jumlah user"""

len(d.user_id.unique())

"""Melihat jumlah kategori"""

len(d.category.unique())

"""Melihat macam-macam brand yang ada"""

d.brand.unique()

"""Melihat jumlah brand"""

len(d.brand.unique())

"""Melihat distribusi rating"""

fig, ax = plt.subplots()
sns.countplot(x='rating', data=d, palette='viridis')
plt.title('Rating')
plt.grid();

"""Melakukan visualisasi pada penujuan gender sebuah item."""

fig, ax = plt.subplots()
sns.countplot(x='model_attr', data=d, palette='viridis')
plt.title('Item Gender')
plt.grid();

"""Distribusi kategori"""

fig, ax = plt.subplots(figsize=(25,6))
sns.countplot(x='category', data=d, palette='viridis')
plt.title('Category')
plt.grid();

"""Distribusi tahun rilis"""

fig, ax = plt.subplots(figsize=(25,6))
sns.countplot(x='year', data=d, palette='viridis')
plt.title('Release Year of Product')
plt.grid();

"""Melihat item dengan frekuensi kemunculan tertinggi"""

d.item_id.value_counts()

"""Melihat user dengan frekuensi kemunculan tertinggi"""

d.user_id.value_counts()

"""# Content-Based Filtering

Memilih data untuk *content-based filtering*. Data yang digunakan ialah item_id, model_attr, category, brand dan year. Selain itu, juga dilakukan pengisian *null values* pada fitur brand, dan melakukan dummy. Selanjutnya dilakukan groubpy dengan teknik rata-rata
"""

f = d[['item_id','model_attr','category','brand','year']]
f.brand = f.brand.fillna('unbranded')
f.year = f.year.astype('str')
f = pd.get_dummies(f)
f = f.groupby('item_id').mean()
f.tail(6)

"""Melakukan pengecekan yang menunjukkan groupby rata-rata berhasil atau tidak, ditunjukkan dengan nilai unik hanya 0 dan 1. Bila terdapat float, maka terindikasi data item tidak konsisten"""

for i in f:
  print(f[i].unique())

"""Melakukan pencarian kesamaan antar item dengan menggunakan cosine_similiarity"""

cosine_sim = cosine_similarity(f)
cosine_sim_df = pd.DataFrame(cosine_sim, index=f.index, columns=f.index)
cosine_sim_df

"""## Recommendation

Membuat fungsi untuk mengeluarkan 10 output rekomendasi. Tidak lupa memilih item secara acak
"""

item_id = pd.Series(f.index).sample(1).iloc[0]

def recommendations(item_id=item_id, threshold=0.7, n_item=10):
  print(f'Showing similar item for item {item_id}')
  print()
  a = np.argsort(cosine_sim_df[item_id].values)
  a = a[~np.isin(a,item_id)]
  a = a[-n_item:][::-1]
  a = {i:cosine_sim_df[item_id][i] for i in a}
  a

  b=0
  for i,j in a.items():
    if j>threshold:
      b+=1
    print('Item', i, '|', 'Cosine Similarity Value :', j)
  print()

  c = b*100/n_item
  print('Precision (%):')
  return c

"""Rekomendasi 10 item yang mirip untuk user yang memilih item 2360 juga, beserta nilai cosine_similiarity-nya"""

recommendations(item_id)

"""## Evaluation

Dengan function yang sama (tanpa output print), buat function turunan untuk mengukur presisi, dimana presisi sistem rekomendasi adalah banyak data yang sesuai dibagi dengan jumlah rekomendasi muncul. Lalu diulangi sejumlah n_to_eval untuk selanjutnya dirata-ratakan.
"""

def _evalRecommendations(item_id=item_id, threshold=0.7, n_item=10):
  a = np.argsort(cosine_sim_df[item_id].values)
  a = a[~np.isin(a,item_id)]
  a = a[-n_item:][::-1]
  a = {i:cosine_sim_df[item_id][i] for i in a}
  a

  b=0
  for i,j in a.items():
    if j>threshold:
      b+=1

  c = b*100/n_item
  return c

"""Fungsi evaluasi yang mengeluarkan output satu per satu selanjutnya dimasukkan ke fungsi loop dan di-loop sebanyak n_to_eval kali, setiap kalinya menggunakan angka item_id sembarang"""

def evalRecommendations(n_to_eval=100):
  a = 0
  for i in range(n_to_eval):
    item_id = pd.Series(f.index).sample(1).iloc[0]
    a = a + _evalRecommendations(item_id)

  print(f'Evaluation (Precision) for {n_to_eval} times try is : {a/n_to_eval} %')

"""Mengeluarkan nilai presisi pada sistem rekomendasi content-based filtering"""

evalRecommendations(100)

"""# Collaborative Filtering

Melakukan pemilihan *attributes* dan target variabel. *Attributes* yang digunakan ialah user_id dan item_id, sedangkan target variabel berupa rating. Selain itu juga dilakukan pembagian data menjadi 80% data latih dan 20% data tes serta menggunakan min-max scaling pada target variabel, dengan catatan fit scaling hanya dilakukan pada data latih
"""

x = d[['user_id','item_id']].values
y = d['rating'].values
x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.8, random_state=42)

min_rating = min(y_train)
max_rating = max(y_train)

minmax = lambda x: (x - min_rating) / (max_rating - min_rating)
y_train = np.array([minmax(x) for x in y_train])
y_val = np.array([minmax(x) for x in y_val])

print(x_train, y_train)

"""Menyimpan jumlah user, item, nilai minimum dan maksimum rating dalam variabel"""

num_users = len(d.user_id.unique())
num_items = len(d.item_id.unique())

print('Number of User: {}, Number of Item: {}, Min Rating Train: {}, Max Rating Train: {}'.format(
    num_users, num_items, min_rating, max_rating
))

"""Merancang arsitektur model, menggunakan teknik embedding, dan ditambahkan pula bias"""

class RecommenderNet(tf.keras.Model):
 
  def __init__(self, num_users, num_items, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_items = num_items
    self.embedding_size = embedding_size

    self.user_embedding = layers.Embedding( # user embedding layer
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1) # bias of user embedding layer

    self.item_embedding = layers.Embedding( # item embedding layer
        num_items,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )

    self.item_bias = layers.Embedding(num_items, 1) # bias of item embedding layer

  def call(self, inputs): # calling the embedding layers
    user_vector = self.user_embedding(inputs[:,0])
    user_bias = self.user_bias(inputs[:, 0])
    item_vector = self.item_embedding(inputs[:, 1])
    item_bias = self.item_bias(inputs[:, 1])
 
    dot_user_item = tf.tensordot(user_vector, item_vector, 2)
 
    x = dot_user_item + user_bias + item_bias

    return tf.nn.sigmoid(x)

"""Memanggil model dan melakukan *compile*. Digunakan optimizer Adam dan metrik RMSE"""

model = RecommenderNet(num_users, num_items, 50)
 
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.0005),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

"""Melakukan set pada callback yang ingin digunakan. Disini digunakan EarlyStopping yang berguna untuk memberhentikan saat tidak ada peningkatan performa dan ModelCheckpoint untuk merekam model saat epoch terbaik"""

early = tf.keras.callbacks.EarlyStopping(monitor='val_root_mean_squared_error', patience=5)

checkpoint = tf.keras.callbacks.ModelCheckpoint(
    'model.tf',
    monitor = 'val_root_mean_squared_error',
    save_best_only = True,
    verbose = 1
)

set_callback = [
                checkpoint,
                early
                ]

"""## Evaluation

Melakukan *running* pada model, dengan epoch sebesar 100 dan batch_size sebesar 1024
"""

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 1024,
    epochs = 100,
    validation_data = (x_val, y_val),
    callbacks=[set_callback]
)

"""Melakukan plot RMSE dan Loss setiap epoch untuk train dan validation data"""

pd.DataFrame(history.history).plot(figsize=(9,6))
plt.xlabel('Epoch')
plt.title('RMSE and Loss Graph')
plt.grid()

"""Load kembali model yang terbaik"""

model = keras.models.load_model('model.tf')

"""Rancang sampel yang ingin diprediksi"""

user_id = d.user_id.sample(1).iloc[0]
item_bought_by_user = d.item_id[d.user_id == user_id].tolist()

a = d.item_id.unique()[~np.isin(d.item_id.unique(), item_bought_by_user)]
item_not_bought = [[i] for i in a]

user_item_array = np.hstack(
    ([[user_id]] * len(item_not_bought), item_not_bought)
)

"""Lakukan prediksi pada user sampel, pada restoran yang belum pernah dikunjunginya"""

ratings = model.predict(user_item_array).flatten()

"""## Recommendation

Keluarkan 10 rekomendasi item selain item yang telah diulas (*review*) untuk user 49390 berdasarkan *collaborative filtering*
"""

print('Showing recommendations for user: {}'.format(user_id))
print('===' * 9)
print('Items with high ratings from user')
print('----' * 8)

a = d[['item_id','rating']][d.user_id == user_id].values

if len(a)>=10:
  user_and_rating = dict(sorted(a, key=lambda item: item[1], reverse=True))
  user_and_rating = dict(list(user_and_rating.items())[:10])
else:
  user_and_rating = dict(sorted(a, key=lambda item: item[1], reverse=True))

for i,j in user_and_rating.items():
    print('Item', int(i), '|', 'Rating :', j)
 
print('----' * 8)
print('Top 10 item recommendation and its rating prediction')
print('----' * 8)
 
top_ratings_indices = {i:ratings[i] for i in ratings.argsort()[-10:][::-1]}
a = {item_not_bought[i][0]:top_ratings_indices[i] for i in top_ratings_indices.keys()}
top_ratings_items = dict(sorted(a.items(), key=lambda item: item[1], reverse=True))

for i,j in top_ratings_items.items():
  j=j*(max_rating-min_rating)+min_rating
  print('Item', int(i), '|', 'Rating Prediction :', '%.2f'%j)